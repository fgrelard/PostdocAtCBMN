:PROPERTIES:
:ID:       b08807ac-d9e3-4987-8b42-be4ec686d94c
:END:
#+title: Deep learning
[[id:6df664eb-63ad-4ef6-af19-bfa17690d3a9][Machine learning]]

* Introduction
** Context
Deep learning is a subset of methods in [[id:6df664eb-63ad-4ef6-af19-bfa17690d3a9][machine learning]], and more precisely [[id:8e1c9185-edd2-4acc-b8a3-08891175b304][supervised machine learning]], which aim at modelizing with a high data abstraction level, with an architecture involving several non-linear transformations.
The architecture involves neural networks, such as [[id:9e280ff8-4335-46b9-b8d8-a5877a0d404b][Convolutional Neural Networks]]. 

Models learn from annotated data. It usually requires a lot of input data.

** History

#+ATTR_ORG: :width 500
#+CAPTION: Evolution of Deep Learning (from [[cite:&He2021]])
[[file:/home/fgrelard/org/fig/captures/yanked_2021-11-25T14_03_58.png]]

_Architecture evolution over time_:
#+ATTR_ORG: :width 500
#+CAPTION: Evolution of the architecture
[[file:/home/fgrelard/org/fig/captures/yanked_2021-11-25T14_06_19.png]]

1) Perceptron [[cite:&Rosenblatt1958]]: inputs $(x_1, \dots, x_n)$ are multiplied by weights $(w_1, \dots, w_n)$ and summed together to generate the output with a step function.
2) [[cite:&Rumelhart1986-learn]]: back-propagation and training of multi layer perceptron (MLP). 
3) [[cite:&Lecun1989-backp]]: First CNN model.
4) [[cite:&Krizhevsky2017-imagen]]: AlexNet, GPU usage, ReLU for activation function.
5) [[cite:&Hu2018-squeez]]: SENet, integrates channel attention layer.
6) [[cite:&Ronneberger2015-u-net]]: UNet: joint learning of localization and classification
7) [[cite:&Goodfellow2014-generative]]: GAN: data generation.

* Concepts
Four components: 
** Deep learning model
Receives an input and outputs an answer.

#+ATTR_ORG: :width 500
#+CAPTION: Main components in a Deep Learning model (from [[cite:&He2021]])
[[file:/home/fgrelard/org/fig/captures/yanked_2021-11-25T15_41_26.png]]


In a CNN model, there are several layers:
*** Convolution
Extracts the feature maps from the inputs. The feature maps consists in convolution kernels with various weights. Those weights are updated during the training stage.
*** Activation
Defines the status of the neuron: activated or non activated.
_Various functions_: sigmoid, ReLU, ELU (exponential linear unit), tanh.
*** Pooling
Subsampling strategy.
_Various strategies_: max pooling, average pooling.
*** Fully connected layers
Optional.

** Loss function
Evaluates the performance of the deep learning model.
Usually denoted $L(w)$, i.e. an implicit function on the model weights.

During training, the prediction is compared with the expected value to compute the loss.
** Optimization algorithm
** Dataset
* _Pros_:
- Architecture suited to extract *complex information* â†’ performs better than other classification algorithms
- *Data-driven nature* suited to process expanding big-data
- Reduce the impact of human errors
