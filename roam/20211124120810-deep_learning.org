:PROPERTIES:
:ID:       b08807ac-d9e3-4987-8b42-be4ec686d94c
:END:
#+title: Deep learning
[[id:6df664eb-63ad-4ef6-af19-bfa17690d3a9][Machine learning]]

* Introduction

** Context
Deep learning is a subset of methods in [[id:6df664eb-63ad-4ef6-af19-bfa17690d3a9][machine learning]], and more precisely [[id:8e1c9185-edd2-4acc-b8a3-08891175b304][supervised machine learning]], which aim at modelizing with a high data abstraction level, with an architecture involving several non-linear transformations.
The architecture involves neural networks, such as [[id:9e280ff8-4335-46b9-b8d8-a5877a0d404b][Convolutional Neural Networks]]. 

Models learn from annotated data. It usually requires a lot of input data.

** History

#+ATTR_ORG: :width 500
#+CAPTION: Evolution of Deep Learning (from [[cite:&He2021]])
[[file:/home/fgrelard/org/fig/captures/yanked_2021-11-25T14_03_58.png]]

_Architecture evolution over time_:
#+ATTR_ORG: :width 500
#+CAPTION: Evolution of the architecture
[[file:/home/fgrelard/org/fig/captures/yanked_2021-11-25T14_06_19.png]]

1) Perceptron [[cite:&Rosenblatt1958]]: inputs $(x_1, \dots, x_n)$ are multiplied by weights $(w_1, \dots, w_n)$ and summed together to generate the output with a step function.
2) [[cite:&Rumelhart1986-learn]]: back-propagation and training of multi layer perceptron (MLP). 
3) [[cite:&Lecun1989-backp]]: First CNN model.
4) [[cite:&Krizhevsky2017-imagen]]: AlexNet, GPU usage, ReLU for activation function.
5) [[cite:&Hu2018-squeez]]: SENet, integrates channel attention layer.
   
   
   
  

* _Pros_:
- Architecture suited to extract *complex information* â†’ performs better than other classification algorithms
- *Data-driven nature* suited to process expanding big-data
- Reduce the impact of human errors
